{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc78d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 16:27:31.404911: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-21 16:27:31.405019: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime, hour, when\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5c2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "spark = SparkSession.builder.appName(\"TelecomActivityPreprocessing\").getOrCreate()\n",
    "data = spark.read.text(\"/user1/data/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf5e09ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|1\\t1383260400000\\t0\\t0.08136262351125882\\t\\t\\t\\t                                                                            |\n",
      "|1\\t1383260400000\\t39\\t0.14186425470242922\\t0.1567870050390246\\t0.16093793691701822\\t0.052274848528573205\\t11.028366381681026|\n",
      "|1\\t1383261000000\\t0\\t0.13658782275823106\\t\\t\\t0.02730046487718618\\t                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e14aff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove lines with empty fields\n",
    "data1 = data.filter((col(\"value\").rlike(r\"\\S\")) & (col(\"value\").rlike(r\"\\S+\\s\\S+\\s\\S+\\s\\S+\\s\\S+\\s\\S+\\s\\S+\\s\\S+\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50e1118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=============>  (13 + 2) / 15][Stage 28:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|1\\t1383260400000\\t39\\t0.14186425470242922\\t0.1567870050390246\\t0.16093793691701822\\t0.052274848528573205\\t11.028366381681026|\n",
      "|1\\t1383261000000\\t39\\t0.27845207746066025\\t0.11992572014174135\\t0.1887771729145041\\t0.13363747203983203\\t11.100963451409388 |\n",
      "|1\\t1383261600000\\t39\\t0.3306414276169333\\t0.1709520296851148\\t0.13417624316013174\\t0.05460092975437236\\t10.892770602791096  |\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data1.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "50d5720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, unix_timestamp, col, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4237fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.withColumn(\"columns\", split(col(\"value\"), \"\\t\")) \\\n",
    "    .select(\n",
    "        col(\"columns\").getItem(0).alias(\"square_id\"),\n",
    "        (col(\"columns\").getItem(1).cast(\"bigint\") / 1000).cast(\"timestamp\").alias(\"timestamp\"),\n",
    "        col(\"columns\").getItem(2).cast(\"int\").alias(\"country_code\"),\n",
    "        col(\"columns\").getItem(3).cast(\"double\").alias(\"sms_in_activity\"),\n",
    "        col(\"columns\").getItem(4).cast(\"double\").alias(\"sms_out_activity\"),\n",
    "        col(\"columns\").getItem(5).cast(\"double\").alias(\"call_in_activity\"),\n",
    "        col(\"columns\").getItem(6).cast(\"double\").alias(\"call_out_activity\"),\n",
    "        col(\"columns\").getItem(7).cast(\"double\").alias(\"internet_traffic_activity\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7534d296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- square_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- sms_in_activity: double (nullable = true)\n",
      " |-- sms_out_activity: double (nullable = true)\n",
      " |-- call_in_activity: double (nullable = true)\n",
      " |-- call_out_activity: double (nullable = true)\n",
      " |-- internet_traffic_activity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9a504d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+-------------------+-------------------+-------------------+--------------------+-------------------------+\n",
      "|square_id|timestamp          |country_code|sms_in_activity    |sms_out_activity   |call_in_activity   |call_out_activity   |internet_traffic_activity|\n",
      "+---------+-------------------+------------+-------------------+-------------------+-------------------+--------------------+-------------------------+\n",
      "|1        |2013-10-31 23:00:00|39          |0.14186425470242922|0.1567870050390246 |0.16093793691701822|0.052274848528573205|11.028366381681026       |\n",
      "|1        |2013-10-31 23:10:00|39          |0.27845207746066025|0.11992572014174135|0.1887771729145041 |0.13363747203983203 |11.100963451409388       |\n",
      "|1        |2013-10-31 23:20:00|39          |0.3306414276169333 |0.1709520296851148 |0.13417624316013174|0.05460092975437236 |10.892770602791096       |\n",
      "+---------+-------------------+------------+-------------------+-------------------+-------------------+--------------------+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "640754e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the data from the current 10-minute interval to 1-hour based on the timestamp\n",
    "data1 = data1.withColumn(\n",
    "    \"hourly_timestamp\",\n",
    "    from_unixtime((unix_timestamp(col(\"timestamp\")) / 3600).cast(\"bigint\") * 3600)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "326b57ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- square_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- sms_in_activity: double (nullable = true)\n",
      " |-- sms_out_activity: double (nullable = true)\n",
      " |-- call_in_activity: double (nullable = true)\n",
      " |-- call_out_activity: double (nullable = true)\n",
      " |-- internet_traffic_activity: double (nullable = true)\n",
      " |-- hourly_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e429d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum call and SMS activity per square_id and timestamp   TO DELETE\n",
    "data1 = data1.agg(  #.groupBy(\n",
    "   # \"square_id\",\n",
    "   # \"hourly_timestamp\"\n",
    "#).agg(\n",
    "    sum(col(\"call_in_activity\") + col(\"call_out_activity\")).alias(\"call_activity\"),\n",
    "    sum(col(\"sms_in_activity\") + col(\"sms_out_activity\")).alias(\"sms_activity\"),\n",
    "    sum(col(\"internet_traffic_activity\")).alias(\"internet_traffic_activity\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd81e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reduce the dataframe size summarizing SMS and call data\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data1 = data1.withColumn('SMS_activity', F.col('sms_in_activity') + F.col('sms_out_activity'))\n",
    "data1 = data1.withColumn('call_activity', F.col('call_in_activity') + F.col('call_out_activity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "467aff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column 'weekend_or_weekday'\n",
    "data1 = data1.withColumn(\n",
    "    \"weekend_or_weekday\",\n",
    "    when(hour(col(\"hourly_timestamp\")).between(0, 5), \"weekend\").otherwise(\"weekday\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "487d62d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- square_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- sms_in_activity: double (nullable = true)\n",
      " |-- sms_out_activity: double (nullable = true)\n",
      " |-- call_in_activity: double (nullable = true)\n",
      " |-- call_out_activity: double (nullable = true)\n",
      " |-- internet_traffic_activity: double (nullable = true)\n",
      " |-- hourly_timestamp: string (nullable = true)\n",
      " |-- SMS_activity: double (nullable = true)\n",
      " |-- call_activity: double (nullable = true)\n",
      " |-- weekend_or_weekday: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5d1f5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.drop('sms_in_activity', 'sms_out_activity', 'call_in_activity', 'call_out_activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eab55de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[square_id: string, timestamp: timestamp, country_code: int, internet_traffic_activity: double, hourly_timestamp: string, SMS_activity: double, call_activity: double, weekend_or_weekday: string]\n"
     ]
    }
   ],
   "source": [
    "print(data1.select(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0f980375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+---------------------+---------------------------------+\n",
      "|weekend_or_weekday|Average_SMS_activity|Average_Call_activity|Average_Internet_traffic_activity|\n",
      "+------------------+--------------------+---------------------+---------------------------------+\n",
      "|           weekday|    7.49561325612599|    8.697384310443645|                  82.180836362139|\n",
      "|           weekend|  2.0342061005246967|   1.0514926201658503|               63.894598042644766|\n",
      "+------------------+--------------------+---------------------+---------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "activity_summary = data1.groupBy('weekend_or_weekday').agg(avg('SMS_activity').alias('Average_SMS_activity'), avg('call_activity').alias('Average_Call_activity'), avg('internet_traffic_activity').alias('Average_Internet_traffic_activity'))\n",
    "activity_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ce1614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# exporting data\n",
    "data1.write.parquet(\"/user1/data/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a5e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=================>                                       (6 + 2) / 20]\r"
     ]
    }
   ],
   "source": [
    "data1.write.csv(\"/user1/data/csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing for modeling (cat values, scaling, splitting)\n",
    "# handle categorical values\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Create StringIndexer for categorical columns\n",
    "indexer = StringIndexer(inputCol='weekend_or_weekday', outputCol='weekend_or_weekday_index')\n",
    "\n",
    "# Apply StringIndexer to get numerical indices\n",
    "indexed_df = indexer.fit(data1).transform(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c26b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply OneHotEncoder to convert indices into binary vectors\n",
    "encoder = OneHotEncoder(inputCols=['weekend_or_weekday_index'],\n",
    "                        outputCols=['weekend_or_weekday_encoded'])\n",
    "encoded_model = encoder.fit(indexed_df)\n",
    "encoded_df = encoded_model.transform(indexed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling numerical features\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Create StandardScaler for numerical columns\n",
    "scaler = StandardScaler(inputCols=['Square_id', 'Timestamp', 'Internet_traffic_activity'],\n",
    "                        outputCols=['Scaled_Square_id', 'Scaled_Timestamp', 'Scaled_Internet_traffic_activity'])\n",
    "\n",
    "# Fit the scaler on the data and transform the features\n",
    "scaled_df = scaler.fit(encoded_df).transform(encoded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9417999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training the autoencoder\n",
    "X_train = preprocessed_data.select(\"*\")  # \"*\" to take all input features\n",
    "# train_data, test_data = scaled_df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236eb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the autoencoder model\n",
    "#input_dim = X_train.shape[1]\n",
    "input_dim = len(X_train.columns)\n",
    "encoding_dim = 32\n",
    "\n",
    "autoencoder = Sequential([\n",
    "    Dense(encoding_dim, activation='relu', input_shape=(input_dim,)),\n",
    "    Dense(input_dim, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the autoencoder model\n",
    "#autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# autoencoder.fit(X_train, X_train, epochs=10, batch_size=32) \n",
    "# ^ this won't work as we can't train a Keras model on a SparkDataframe \n",
    "# and data is too big to be converted to a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e19365",
   "metadata": {},
   "source": [
    "Since we cannot directly fit a Keras model on a Spark DataFrame, we need to convert it to an RDD (Resilient Distributed Dataset). We use the rdd property of the Spark DataFrame to achieve this. <br><br>\n",
    "\n",
    "We use the foreachPartition method of the RDD to train the autoencoder model on each partition of the data. This allows to distribute the training process across multiple workers in the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba270cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = X_train.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1afc266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the autoencoder model on each partition\n",
    "def train_autoencoder(partition):\n",
    "\n",
    "    # Create a new autoencoder model for each partition\n",
    "    autoencoder = Sequential([\n",
    "        Dense(encoding_dim, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(input_dim, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile and fit the autoencoder model on the partition data\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(partition, partition, epochs=10, batch_size=32)\n",
    "\n",
    "# Train the autoencoder model on each partition of the RDD\n",
    "rdd.foreachPartition(train_autoencoder)\n",
    "\n",
    "# Detect anomalies using the trained autoencoder\n",
    "reconstructed_data = autoencoder.predict(X_train)\n",
    "mse = tf.keras.losses.mean_squared_error(X_train, reconstructed_data)\n",
    "anomaly_threshold = mse.mean() + mse.std() * 2  # Adjust the threshold as per your requirements\n",
    "\n",
    "anomalies = X_train[mse > anomaly_threshold]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73eaf0",
   "metadata": {},
   "source": [
    "To measure the performance of the autoencoder model, we can use the reconstruction error as a metric. The reconstruction error is typically measured as the mean squared error (MSE) between the original input and its reconstruction. A lower reconstruction error indicates better performance.\n",
    "\n",
    "To evaluate the modelâ€™s performance, we can calculate the MSE between the original input data X_train and its corresponding reconstructed data reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1064251",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.mean_squared_error(X_train, reconstructed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting anomalies versus normal items\n",
    "plt.scatter(X_train['Timestamp'], X_train['SMS_in_activity'], label='Normal')\n",
    "plt.scatter(anomalies['Timestamp'], anomalies['SMS_in_activity'], color='red', label='Anomaly')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('SMS-in Activity')\n",
    "plt.title('Anomalies Detected')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f466440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d346234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build multiple autoencoder models with different initializations or hyperparameters\n",
    "num_models = 5\n",
    "autoencoders = []\n",
    "for _ in range(num_models):\n",
    "    autoencoder = Sequential([\n",
    "        Dense(encoding_dim, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(input_dim, activation='sigmoid')\n",
    "    ])\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoders.append(autoencoder)\n",
    "\n",
    "# Train each autoencoder model separately\n",
    "for autoencoder in autoencoders:\n",
    "    autoencoder.fit(X_train, X_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Combine the outputs of all autoencoder models using ensemble methods (e.g., averaging)\n",
    "reconstructed_data = np.mean([autoencoder.predict(X_train) for autoencoder in autoencoders], axis=0)\n",
    "\n",
    "# Calculate the mean squared error (MSE) between the original input and ensemble reconstruction\n",
    "mse = tf.keras.losses.mean_squared_error(X_train, reconstructed_data)\n",
    "\n",
    "# Set a threshold based on your specific requirements to classify data points as anomalies or normal\n",
    "anomaly_threshold = mse.mean() + mse.std() * 2\n",
    "\n",
    "# Identify anomalies based on the MSE threshold\n",
    "anomalies = X_train[mse > anomaly_threshold]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
